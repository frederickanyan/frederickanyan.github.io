---
title: 'Association Between Two Categorical Variables'
author: "Frederick Anyan"
date: 2023-06-15
categories: [Categorical variables]
---
We can analyse the association between two categorical variables in several ways using bar graphs or pie charts. The most common way is to use contingency tables with rows and columns displaying categories of each variable. The count or frequency of each pair of categories are given in a cell. 

I will show how you can analyse the association between two categorical variables using different functions in different packages. The CrossTbale() function seems to be very popular as is the chisq.test() function. My preference is the ggbarstats() function in the 'ggstatsplot' package. I highly recommend this package as it can produce beautiful plots and charts that are publication-ready with good quality as you can find below. And just by plotting your pie chart or stacked bar graph, the ggpiestats() and the ggbarstats() function also compute the Chi-Square test statistic, with very useful information in addition, including a Bayesian effect size. 

In this example, we will answer the research question: is family income associated with happiness? 
These questions are asked in the General Social Survey (GSS).
<ul>
 <li>"Compared to American families in general, would you say that your family income is below average, average, or above average?".</li> 
 <li>"Taken all together, would you say that you are very happy, pretty happy, or not too happy?".</li>
</ul>

We start with the null hypothesis that:
<ul>
  <li>There is no association between family income and happiness.</li>
</ul>

To test the null hypothesis, we can use the CrossTable() function in the 'gmodels' package. Note that the CrossTable() can take both raw data and contingency table. 

```{r}
data <- read.csv("/Volumes/anyan-1/happinesGSS12_long.csv")# Read the data for the analysis 
```


```{r}
library(gmodels) #Load the packgae to access the CrossTable() function
CrossTable(data$Income, data$Happy) #Get a cross tabulation of variables with cell counts/contingency table
```

Frequencies are displayed in each cell. The frequencies or cell counts in each cell can be converted to percentages (N/Row Total). Within each category of income, the percentage for the three categories of happiness is displayed. Thus, of the 342 people who reported their family income as above average, 135 reported themselves as very happy, which gives us a proportion of 39.5%, rounded to 40% (i.e., 135/342). By contrast, 119 (22%) of the 537 people who reported below average family income, reported they were very happy. 

These percentages are called *conditional percentages* and show the distribution of proportions in one variable, conditional on the other variable - hence, also called conditional distributions. For example, the *conditional distribution* of happiness for those who reported average income are 10%, 58%, and 32%. They can also be interpreted as *conditional probabilities*. Given that an individual reported average family income, the probability of not being happy, or pretty happy, or very happy is 10%, 58%, and 32%, respectively.

Percentages in the row labelled 'Column Total' are called *marginal proportion*, as are those in the Row Total. The marginal proportion of people who reported not happy, pretty happy, or very happy is 12%, 57% and 31%, respectively. Similarly, the marginal proportion of people who reported below average income, average income and above average income is 31%, 49% and 20%, respectively. 

To perform a Chi-Square test with the CrossTable() function, we can add some arguments to the code we previously used. 
```{r}
CrossTable(data$Income, data$Happy, 
           #fisher = TRUE, #To get fisher's exact test 
           chisq = TRUE, #To get pearson chi-square statistic
           expected = TRUE, #To get the expected cell counts (which should be > 5 in each cell)
           ) 
```
Here we have the chi-square statistic and corresponding p-value for the test of independence under the null hypothesis that, income and happiness are independent (no association). Since the p-value is very small (< .05), we can reject the null hypothesis in favor of the alternative hypothesis. Thus, there is an association between family income and happiness and that this may not be due to chance or sampling variability. As such, we can conclude that there is an association between family income and happiness in the population. [Find out why this conclusion could be problematic for statistical inference](../quantposts/null hypothesis testing/index.qmd) 

We can also use a different function to perform the Chi-Square test. Here, we use the chisq.test() function in the 'stats' package, which also contains many other functions.
```{r}
chi.test <- chisq.test(data$Income, data$Happy)
chi.test 
```

```{r}
chi.test$expected #To get expected cell counts
```

```{r}
chi.test$stdres #Assuming there is a significant result, the residuals/standardized residuals can help us to understand the patterns of association
```
The residuals show the difference between the observed counts and what the model predicts (the expected counts/frequencies) in a particular cell computed under the null hypothesis. The standardized residual is similar to the z-score and indicates the number of standard errors that an observed count falls from its expected count. If the value of the standardized residual lies outside of +/-1.96, then it is significant at p < .05; if it lies outside +/-2.58, then it is significant at p <. 01 and when outside +/-3.29, then it is significant at p < .001. 

For example, if we take the first cell of people who reported above average family income, but not happy, the observed count is -2.49 standard deviations (SD) below the expected count for this particular cell. Alternatively, for people with above average family income, many fewer were not happy than what the model predicted or, in other words, what the assumption of independence between income and happiness would predict. 


We can interpret the positive standardized residuals first, followed by the negative residuals.
<ul>
  <li>For standardized residuals 5.83 and 3.96, the observed count is much higher than the expected count (> 3 standard           deviations). For people who reported below average family income, many more were not happy whereas for people who reported      above average family income, many more were very happy.</li>

  <li>For standardized residuals -3.41, -2.02 and -5.13 the observed count is much lower than the expected count (< 2 standard    deviations, if we use p < .05). For people who reported average family income, much fewer were not happy. For people who        reported below average family income, much fewer were very happy.</li>

  <li>Overall, the association between income and happiness seems to be driven by average to above average family income, and     much more so, by above average family income.</li>
  </ul>
The chi-square test does not tell us where in the cell counts or frequencies the difference from independence lies, that is why the standardized residuals are useful for interpretation and understanding the patterns of association between the variables. 

===
We can use the 'ggstatsplot' package for this exercise. This package "creates graphics with details from statistical tests included in the plots themselves. It provides an easier syntax to generate information-rich plots for statistical analysis of continuous (violin plots, scatterplots, histograms, dot plots, dot-and-whisker plots) or categorical (pie and bar charts) data". [See here](https://cran.r-project.org/web/packages/ggstatsplot/index.html)

```{r}
library(ggstatsplot) #Load the package to access the functions
```

By plotting a pie chart, we also get the results for the Chi-Square test
```{r}
ggpiestats(data = data,
           x = Income,
          y = Happy,
          label = "both")
```

We can also perform and visualize the Chi-Square test of independence using the ggbarstats() function when we call the function to plot a stacked bar graph.
```{r}
ggbarstats(data = data,
           x = Income,   #The variable to use as the rows in the contingency table
          y = Happy,     #The variable to use as the columns in the contingency table.
          label = "both") #You can use "percentage" or "count"
```
We can see that the proportion of people who reported below average family income reduced with increasing happiness whereas the proportion of people who reported above average family income increases with increasing happiness. This support the previous conclusion that, the association between income and happiness seems to be driven by above average family income.

Another measure of effect size is the Cramer's V with its 95% Confidence Interval next to the p-value. The effect size of .12 is a relatively weak association. This function also provides a corresponding Bayesian effect size, which is much more useful. There is also a Bayes factor, which tests both the null and alternative hypothesis at the same time. A Bayes factor of -16.02 indicates a decisive evidence in favor of the alternative hypothesis that there is an association between family income and happiness. 

Finally, this function shows the proportion test for family income (below average, average and above average) in each category of happiness (not happy, pretty happy, and very happy) under the null hypothesis that the sample proportions are not equal. They show whether the proportions inside each category of happiness differ. Since the p-values are all very small, we can conclude that the sample proportions of family income within each category of happiness differs.












